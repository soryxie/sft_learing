{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/songrun/miniconda3/envs/fine-tune/lib/python3.12/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "/home/songrun/miniconda3/envs/fine-tune/lib/python3.12/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "/home/songrun/miniconda3/envs/fine-tune/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/songrun/miniconda3/envs/fine-tune/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/songrun/miniconda3/envs/fine-tune/lib/python3.12/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/songrun/miniconda3/envs/fine-tune/lib/python3.12/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      " 52%|█████▏    | 25/48 [05:30<05:11, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1708, 'learning_rate': 6.756250000000001e-06, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [10:38<00:00, 13.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 638.0823, 'train_samples_per_second': 39.18, 'train_steps_per_second': 0.075, 'train_loss': 4.168620586395264, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# command-single = \"\"\"\n",
    "# python3 script.py --model_name EleutherAI/gpt-neo-125m --dataset_name imdb \\\n",
    "#     --batch_size 16 \\\n",
    "#     --dataset_text_field text --seq_length 512 --load_in_8bit True --use_peft True \\\n",
    "#     --peft_lora_r 64 --peft_lora_alpha 16 --gradient_checkpointing True \\\n",
    "#     --use_auth_token False \n",
    "# \"\"\"\n",
    "\n",
    "command_multi = \"\"\"\n",
    "accelerate launch --config_file=./accelerate_configs/multi_gpu.yaml --num_processes 2 \\\n",
    "    script.py --model_name EleutherAI/gpt-neo-125m --dataset_name imdb \\\n",
    "        --batch_size 16 \\\n",
    "        --dataset_text_field text --seq_length 512 --load_in_4bit True --use_peft True \\\n",
    "        --peft_lora_r 64 --peft_lora_alpha 16 \\\n",
    "        --use_auth_token False --logging_steps 25 --num_train_epochs 1 \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.system(command_multi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
